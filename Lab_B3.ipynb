{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import v2\n",
        "import torchvision.utils as utils\n",
        "import copy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "import ml_library as ML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = {\n",
        "    'model': str,\n",
        "    'batch_size': int,\n",
        "    'num_workers': int,\n",
        "    'learning_rate': float,\n",
        "    'momentum': None,\n",
        "    'optimizer': str,\n",
        "    'training_loss': [],\n",
        "    'validation_loss': [],\n",
        "    'training_accuracy': [],\n",
        "    'validation_accuracy': []\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nLoTITdis1y"
      },
      "source": [
        "# **Lab B3:** Medical Imaging Analysis\n",
        "\n",
        "In this lab you will set up, and train a model that seeks to perform classification of medical OCT scans. In the data there are four classes of scans: Healthy, CNV, DME and DRUSEN. The last three are eye diseases that cause visible damage to the retina and can be spotted through the OCT scans.\n",
        "\n",
        "Your job is to set up the tranforms, the model, fit function, test function and the parameters. You must also tune the parameters and add more transforms to reach best performance possible.\n",
        "\n",
        "IMPORTANT: The data is divided in test, validation and train sets. Make sure to use all three sets properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classes = [0, 1, 2, 3]\n",
        "class_labels = ['CNV', 'DME', 'DRUSEN', 'NORMAL']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usXliL3gi6Xx"
      },
      "outputs": [],
      "source": [
        "path_to_data = 'data/'\n",
        "path_train = path_to_data + 'train'\n",
        "path_test = path_to_data + 'test'\n",
        "path_val = path_to_data + 'val'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Source:\n",
        "\n",
        "- https://debuggercafe.com/pytorch-imagefolder-for-training-cnn-models/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLw-EtRpixSd"
      },
      "source": [
        "**First Step:** Load the Data, setup Transforms, and Initialize the Dataloader\n",
        "\n",
        "\n",
        "[Here](https://pytorch.org/vision/stable/transforms.html) is a great resource to learn more about the different transforms that can be added. The goal of the transform is to properly prepare the data to be sent to the model and to add data augmentation. You may have pictures of different resolution sizes, so here is a good time to set a transform to make the sizes of images uniform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnD-fLlbin-E"
      },
      "outputs": [],
      "source": [
        "size = (224, 224)\n",
        "\n",
        "# Transforms for train set\n",
        "train_transform = v2.Compose([\n",
        "    v2.RandomResizedCrop(size=size, antialias=True),\n",
        "    # v2.Resize(size=size),\n",
        "    v2.RandomHorizontalFlip(p=0.5),\n",
        "    v2.RandomVerticalFlip(p=0.5),\n",
        "    v2.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
        "    # v2.ElasticTransform(),\n",
        "    # v2.RandomRotation(degrees=(30, 70)),\n",
        "    v2.ToTensor(),\n",
        "    # v2.ToDtype(torch.float32, scale=True),\n",
        "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Transforms for test/val set\n",
        "test_val_transform = v2.Compose([\n",
        "    v2.Resize(size=size),\n",
        "    v2.ToTensor(),\n",
        "    # v2.ToDtype(torch.float32, scale=True),\n",
        "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "                                 ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_train = ImageFolder(root=path_train, transform=train_transform)\n",
        "dataset_test = ImageFolder(root=path_test, transform=test_val_transform)\n",
        "dataset_val = ImageFolder(root=path_val, transform=test_val_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Batch Size\n",
        "batch_size = 32\n",
        "\n",
        "# Number of workers\n",
        "num_workers = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCQzmL5wi4Sq"
      },
      "outputs": [],
      "source": [
        "#Initialize dataloader for train set\n",
        "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size = batch_size, shuffle=True, \n",
        "                                             pin_memory = True,\n",
        "                                             num_workers=num_workers,\n",
        "                                             persistent_workers=True)\n",
        "\n",
        "#Initialize dataloader for test set\n",
        "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size = batch_size, shuffle=False)\n",
        "\n",
        "#Initialize dataloader for val set\n",
        "val_loader = torch.utils.data.DataLoader(dataset_val, batch_size = batch_size, shuffle=True)\n",
        "\n",
        "kfold_dataset = torch.utils.data.ConcatDataset([dataset_train, dataset_val])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function to show a batch\n",
        "def show_batch(sample_batched):\n",
        "    images_batch, labels_batch = \\\n",
        "            sample_batched[0], sample_batched[1]\n",
        "\n",
        "    grid = utils.make_grid(images_batch)\n",
        "    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
        "    print(' '.join('%d' % labels_batch[j] for j in range(batch_size)))\n",
        "\n",
        "for i_batch, sample_batched in enumerate(train_loader):\n",
        "    print(i_batch, sample_batched[0].size(),\n",
        "          sample_batched[1].size())\n",
        "\n",
        "    # observe 4th batch and stop.\n",
        "    if i_batch == 3:\n",
        "        plt.figure()\n",
        "        show_batch(sample_batched)\n",
        "        plt.axis('off')\n",
        "        plt.ioff()\n",
        "        plt.show()\n",
        "\n",
        "        \n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "images_batch, labels_batch = \\\n",
        "            sample_batched[0], sample_batched[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ETEYXSxjJ9e"
      },
      "source": [
        "**Second Step:** Design Model's Architecture and code it here in with PyTorch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnsRWWwgjJLN"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "# Instantiate Pretrained ResNet\n",
        "# model = models.resnet50()\n",
        "model = models.resnet18()\n",
        "\n",
        "# Modify final layer\n",
        "# model.fc = nn.Linear(in_features=2048, out_features=(len(classes)), bias=True)\n",
        "model.fc = nn.Linear(in_features=512, out_features=(len(classes)), bias=True)\n",
        "\n",
        "# Load onto GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# View Model to validate\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcFDnokrjQ7h"
      },
      "source": [
        "**Third Step:** Code Fit and Test functions. This is similar to Lab 3, but this time make sure to use the validation set as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define in `ml_library.py`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwH0Q4BrjXBy"
      },
      "source": [
        "**Fourth Step:** Set Parameters and run model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define Parameters\n",
        "num_epochs = 20\n",
        "lr = 0.0001\n",
        "momentum = 0.9\n",
        "hist = copy.deepcopy(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define Loss Fuction\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define Optimizer\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "# hist['momentum'] = momentum\n",
        "hist['optimizer'] = 'Adam'\n",
        "\n",
        "hist['model'] = 'ResNet18'\n",
        "hist['batch_size'] = batch_size\n",
        "hist['num_workers'] = num_workers\n",
        "hist['learning_rate'] = lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# hist = ML.train(model=model, loss_fn=loss_function, optimizer=optimizer, \n",
        "#                 train_loader=train_loader, test_loader=val_loader, num_epochs=num_epochs,\n",
        "#                 device = device, history=hist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train model with K-Fold Cross Validation\n",
        "k_folds = 5\n",
        "\n",
        "hist = ML.kFoldCrossValTrain(model=model, loss_fn=loss_function, optimizer=optimizer, dataset=kfold_dataset, num_epochs=num_epochs, k_folds=k_folds, device=device, history=hist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot Learning Curves\n",
        "ML.plot_learning_curve(hist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('last validation data accuracy', hist['validation_accuracy'][-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('highest validation accuracy achieved: ' , max(hist['validation_accuracy']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hist['validation_accuracy'].index(max(hist['validation_accuracy']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions, test_labels = ML.predict(model=model, test_loader=test_loader, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cm = confusion_matrix(test_labels, predictions, normalize='all')\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "disp.plot(include_values=True, xticks_rotation='vertical', ax=ax)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('test data accuracy: ', ML.test_accuracy(model=model, data_loader=test_loader, device=device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9KLkX97mN8l"
      },
      "outputs": [],
      "source": [
        "##### LAST STEP, SAVE MODEL\n",
        "path_model_save = #choose path to save model\n",
        "torch.save(net.state_dict(), path_model_save)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFz5xU9b6xtx"
      },
      "source": [
        "**Fifth Step:** Model interpretability.\n",
        "\n",
        "For this assignment, you will interpret the model's results through the use of saliency mapping. You will use the following package: [GitHub](https://github.com/jacobgil/pytorch-grad-cam).\n",
        "\n",
        "You are expected to install the package on your environment and go through the GitHub to learn its application. Below is an example code to help you get started:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k71WRu0v6124"
      },
      "outputs": [],
      "source": [
        "### Sample Use with ResNet50:\n",
        "\n",
        "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "model = resnet50(pretrained=True)\n",
        "target_layers = [model.layer4[-1]]\n",
        "\n",
        "\n",
        "input_tensor = # Your input data\n",
        "\n",
        "\n",
        "# Note: input_tensor can be a batch tensor with several images!\n",
        "\n",
        "# Construct the CAM object once, and then re-use it on many images:\n",
        "cam = GradCAM(model=model, target_layers=target_layers, use_cuda=False)\n",
        "\n",
        "targets = ### your label\n",
        "\n",
        "# You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\n",
        "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
        "\n",
        "# In this example grayscale_cam has only one image in the batch:\n",
        "grayscale_cam = grayscale_cam[0, :]\n",
        "visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
